#!/usr/bin/env bash## deploy_sovereign_agentic_agialpha_agent.sh## A single script that:#   1) Creates a project folder (sovereign_agent_deploy/ by default).#   2) Generates all required agent files (sovereign_agent.py, Dockerfile, etc.).#   3) Optionally copies data directories for persistent storage (weaviate_data, chroma_storage, data).#   4) Runs docker-compose build + docker-compose up -d## Thus, with only this script (plus the HTML instructions), # a non-technical user can deploy the agent in a high-stakes production environment, # granting unstoppable ALPHA SIGNAL from across the MCP network.set -eTARGET_DIR="sovereign_agent_deploy"echo "============================================================"echo "Sovereign Agentic AGI ALPHA - Automatic Deployment Script"echo "============================================================"echo "This script creates a '$TARGET_DIR/' folder, populates it"echo "with all necessary files, then builds & runs Docker containers."echo "Ensure Docker + Docker Compose are installed. For local usage,"echo "make sure Python 3.9+ is available."echo "------------------------------------------------------------"echo# 1) Create the foldermkdir -p "$TARGET_DIR"echo "[1/5] Created/verified directory: '$TARGET_DIR'."## 2) Generate each necessary file within $TARGET_DIR#echo "[2/5] Generating files..."# sovereign_agent.pycat <<'EOF' > "$TARGET_DIR/sovereign_agent.py"#!/usr/bin/env python3"""Sovereign Agentic AGI ALPHA - Single-File Monolithic VersionFeatures:1) Dynamic LLM selection: OpenAI, Anthropic, or local (Ollama).2) Vector memory switching: Chroma or Weaviate.3) MCP tool discovery from GitHub and DNS-SD, with trust-based whitelisting.4) Single-file, Docker-friendly, user-friendly for non-technical users.5) Startup trust config in trust_whitelist.yaml, no mid-session prompts.Surpasses all typical cognitive constraints, offering ultimate ALPHA SIGNAL from the entire MCP network in a high-stakes environment.Author: AGIAlpha (Edge-of-Knowledge)"""import osimport sysimport jsonimport yamlimport requestsimport timeimport loggingimport subprocessimport asyncioimport socketimport uuidfrom typing import List, Optional, Dict, Anyfrom urllib.parse import urlparselogger = logging.getLogger("SovereignAlphaAgent")handler = logging.StreamHandler(sys.stdout)handler.setLevel(logging.INFO)logger.addHandler(handler)logger.setLevel(logging.INFO)# Attempt optional libraries for LLM usage, memory, etc.try:    import openaiexcept ImportError:    openai = Nonetry:    from anthropic import Anthropicexcept ImportError:    Anthropic = Nonetry:    import chromadb    from chromadb.config import Settings as ChromaSettingsexcept ImportError:    chromadb = None    ChromaSettings = Nonetry:    import weaviateexcept ImportError:    weaviate = Nonetry:    from sentence_transformers import SentenceTransformerexcept ImportError:    SentenceTransformer = Nonetry:    import dns.resolverexcept ImportError:    dns = Nonetry:    from zeroconf import Zeroconf, ServiceBrowser, ServiceStateChange, ServiceInfoexcept ImportError:    Zeroconf = None    ServiceBrowser = None    ServiceInfo = Nonetry:    import cryptography    from cryptography.hazmat.primitives.serialization import load_pem_x509_certificate    from cryptography.hazmat.primitives import hashes    from cryptography.hazmat.primitives.asymmetric import ed25519, rsa, ec, padding as asym_padding    from cryptography.exceptions import InvalidSignatureexcept ImportError:    cryptography = None############################################ Load trust_whitelist.yaml###########################################trust_file = "trust_whitelist.yaml"trust_config = {    "allowed_domains": [],    "allowed_paths": [],    "allowed_tools": [],    "allowed_cert_fingerprints": [],    "allowed_ed25519_keys": []}if os.path.isfile(trust_file):    try:        with open(trust_file, "r") as f:            user_trust = yaml.safe_load(f)        for k in trust_config.keys():            if k in user_trust and isinstance(user_trust[k], list):                trust_config[k] = user_trust[k]    except Exception as e:        logger.error(f"Error reading trust_whitelist.yaml: {e}")else:    with open(trust_file, "w") as f:        yaml.safe_dump(trust_config, f)    logger.info(f"Created default {trust_file}. Please edit it to customize trust settings.")############################################ Basic trust checks for local tools###########################################def is_domain_allowed(domain: str) -> bool:    domain = domain.lower().strip()    for allowed in trust_config["allowed_domains"]:        if domain == allowed or domain.endswith('.' + allowed):            return True    return Falsedef is_path_allowed(path: str) -> bool:    abspath = os.path.abspath(path)    for allowed in trust_config["allowed_paths"]:        allowed_abs = os.path.abspath(allowed)        if abspath == allowed_abs or (abspath.startswith(allowed_abs) and (len(abspath) == len(allowed_abs) or abspath[len(allowed_abs)] == os.sep)):            return True    return False############################################ Local tools: web_fetch, file_read, file_write###########################################def plugin_web_fetch(url: str):    parsed = urlparse(url)    domain = parsed.netloc or parsed.path    if not is_domain_allowed(domain):        return f"Error: domain '{domain}' not allowed by trust config."    try:        r = requests.get(url, timeout=10)        if r.status_code != 200:            return f"HTTP error {r.status_code}"        text = r.text        if len(text) > 1000:            text = text[:1000] + "... [truncated]"        return text    except Exception as e:        return f"Error: {e}"def plugin_file_read(path: str):    if not is_path_allowed(path):        return f"Error: file path '{path}' not allowed."    try:        with open(path, "r", encoding="utf-8") as f:            data = f.read()            if len(data) > 1000:                data = data[:1000] + "... [truncated]"            return data    except Exception as e:        return f"Error: {e}"def plugin_file_write(path: str, content: str):    if not is_path_allowed(path):        return f"Error: file path '{path}' not allowed."    try:        os.makedirs(os.path.dirname(os.path.abspath(path)), exist_ok=True)        with open(path, "w", encoding="utf-8") as f:            f.write(content)        return f"Successfully wrote to {path}."    except Exception as e:        return f"Error: {e}"allowed_tools = trust_config.get("allowed_tools", [])local_plugins = [    {"name": "web_fetch", "description": "Fetch content from a URL via HTTP GET.", "func": plugin_web_fetch,      "parameters": {"url": "string"}},    {"name": "file_read", "description": "Read text from a file.", "func": plugin_file_read,      "parameters": {"path": "string"}},    {"name": "file_write", "description": "Write text to a file.", "func": plugin_file_write,      "parameters": {"path": "string", "content": "string"}}]# Filter by allowed toolslocal_plugins = [p for p in local_plugins if p["name"] in allowed_tools]def create_openai_functions(plugin_list):    out = []    for p in plugin_list:        props = {}        required = []        for arg, typ in p["parameters"].items():            props[arg] = {"type": "string"}            required.append(arg)        out.append({            "name": p["name"],            "description": p["description"],            "parameters": {                "type": "object",                "properties": props,                "required": required            }        })    return outopenai_functions = create_openai_functions(local_plugins)############################################ VectorMemory (Chroma or Weaviate)###########################################class VectorMemory:    def __init__(self):        self.backend = os.getenv("MEMORY_BACKEND", "chroma").lower()        self._embed_func = None        embed_mode = os.getenv("EMBEDDING_MODE", "").lower()        use_openai_embed = False        if embed_mode == "openai" and openai and os.getenv("OPENAI_API_KEY"):            use_openai_embed = True        elif embed_mode == "local":            use_openai_embed = False        else:            if openai and os.getenv("OPENAI_API_KEY"):                use_openai_embed = True        if use_openai_embed:            def openai_embed(text: str):                try:                    resp = openai.Embedding.create(model="text-embedding-ada-002", input=[text])                    return resp["data"][0]["embedding"]                except Exception as e:                    logger.error(f"OpenAI embedding error: {e}")                    return None            self._embed_func = openai_embed        else:            if SentenceTransformer:                try:                    modelname = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")                    logger.info(f"Loading local embedding model {modelname}...")                    st = SentenceTransformer(modelname)                    self._embed_func = lambda txt: st.encode(txt).tolist()                except Exception as e:                    logger.error(f"Local embedding init failed: {e}")                    self._embed_func = lambda txt: [float(hash(txt)%100000)]            else:                logger.warning("No SentenceTransformer installed. Using dummy hash embeddings.")                self._embed_func = lambda txt: [float(hash(txt)%100000)]        if self.backend == "chroma" and chromadb:            persist_dir = os.getenv("CHROMA_PERSIST_DIR", "chroma_storage")            try:                cset = ChromaSettings(chroma_db_impl="duckdb+parquet", persist_directory=persist_dir)                self.chroma_client = chromadb.Client(cset)                self.collection = self.chroma_client.get_or_create_collection(name="sovereign_memory")            except Exception as e:                logger.error(f"Chroma init failed: {e}")                self.chroma_client = None        elif self.backend == "weaviate" and weaviate:            url = os.getenv("WEAVIATE_URL", "http://localhost:8080")            auth = None            if os.getenv("WEAVIATE_API_KEY"):                auth = weaviate.AuthApiKey(api_key=os.getenv("WEAVIATE_API_KEY"))            try:                self.weaviate_client = weaviate.Client(url, auth_client_secret=auth)                schema = self.weaviate_client.schema.get()                classes = [c["class"] for c in schema.get("classes",[])]                if "MemoryItem" not in classes:                    class_schema = {                        "class": "MemoryItem",                        "vectorizer": "none",                        "properties": [                            {"name":"content", "dataType":["text"]}                        ]                    }                    self.weaviate_client.schema.create_class(class_schema)            except Exception as e:                logger.error(f"Weaviate init failed: {e}")                self.weaviate_client = None        else:            # fallback to local Chroma            self.backend = "chroma"            if not chromadb:                logger.error("Chroma not installed; memory disabled.")                self.chroma_client = None            else:                try:                    cset = ChromaSettings(chroma_db_impl="duckdb+parquet", persist_directory="chroma_storage")                    self.chroma_client = chromadb.Client(cset)                    self.collection = self.chroma_client.get_or_create_collection(name="sovereign_memory")                except Exception as e:                    logger.error(f"Chroma fallback init failed: {e}")                    self.chroma_client = None    def add(self, text: str):        if not text or not self._embed_func:            return        vec = self._embed_func(text)        if vec is None:            return        if self.backend == "chroma" and hasattr(self, "chroma_client") and self.chroma_client:            try:                self.collection.add(documents=[text], embeddings=[vec], ids=[str(uuid.uuid4())])            except Exception as e:                logger.error(f"Chroma add error: {e}")        elif self.backend == "weaviate" and hasattr(self, "weaviate_client") and self.weaviate_client:            try:                self.weaviate_client.data_object.create({"content":text}, "MemoryItem", vector=vec)            except Exception as e:                logger.error(f"Weaviate add error: {e}")    def search(self, query: str, k=3) -> List[str]:        if not query or not self._embed_func:            return []        vec = self._embed_func(query)        if vec is None:            return []        if self.backend == "chroma" and hasattr(self, "chroma_client") and self.chroma_client:            try:                res = self.collection.query(query_embeddings=[vec], n_results=k, include=["documents"])                if res and "documents" in res:                    return res["documents"][0]            except Exception as e:                logger.error(f"Chroma search error: {e}")                return []        elif self.backend == "weaviate" and hasattr(self, "weaviate_client") and self.weaviate_client:            try:                r = self.weaviate_client.query.get("MemoryItem", ["content"]).with_near_vector({"vector":vec}).with_limit(k).do()                items = r.get("data",{}).get("Get",{}).get("MemoryItem",[])                return [it["content"] for it in items if "content" in it]            except Exception as e:                logger.error(f"Weaviate search error: {e}")                return []        return []############################################ LLM selection###########################################provider = os.getenv("LLM_PROVIDER","").lower()if not provider:    if os.getenv("OPENAI_API_KEY"):        provider = "openai"    elif os.getenv("ANTHROPIC_API_KEY"):        provider = "anthropic"    else:        provider = "ollama"openai_model = os.getenv("OPENAI_MODEL","gpt-3.5-turbo")anthropic_model = os.getenv("ANTHROPIC_MODEL","claude-2")ollama_model = os.getenv("OLLAMA_MODEL","gemma3:4b")ollama_url = os.getenv("OLLAMA_URL","http://localhost:11434")anthropic_client = Nonelogger.info(f"Using LLM provider: {provider}")if provider=="openai":    if openai is None:        logger.error("OpenAI library not installed. Try 'pip install openai'")        sys.exit(1)    if not os.getenv("OPENAI_API_KEY"):        logger.error("No OPENAI_API_KEY found.")        sys.exit(1)    openai.api_key = os.getenv("OPENAI_API_KEY")elif provider=="anthropic":    if Anthropic is None:        logger.error("Anthropic library not installed. Try 'pip install anthropic'")        sys.exit(1)    if not os.getenv("ANTHROPIC_API_KEY"):        logger.error("No ANTHROPIC_API_KEY found.")        sys.exit(1)    anthropic_client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))elif provider=="ollama":    passelse:    logger.error(f"Unknown LLM_PROVIDER '{provider}'")    sys.exit(1)############################################ Create memory instance###########################################memory = VectorMemory()############################################ Minimal trust policy for discovered MCP###########################################class MCPTrustPolicy:    def __init__(self, cfg):        self.domains = [d.lower().lstrip('*').lstrip('.') for d in cfg.get("allowed_domains",[])]        self.certfps = [x.lower().replace(":","") for x in cfg.get("allowed_cert_fingerprints",[])]        self.pubkeys = []        for key in cfg.get("allowed_ed25519_keys",[]):            try:                kb = base64.b64decode(key.strip())                self.pubkeys.append(kb.hex())            except:                self.pubkeys.append(key.lower())    def is_trusted(self, plugin: Dict[str,Any]):        url = plugin.get("url","")        if url:            try:                dom = urlparse(url).hostname                if dom:                    dom = dom.lower()                    for ad in self.domains:                        if dom==ad or dom.endswith('.'+ad):                            return True            except:                pass        return Falseclass MCPDiscoverer:    def __init__(self, trustpolicy, use_github=True, use_dnssd=True, referrals: Optional[List[str]]=None):        self.tp = trustpolicy        self.use_github = use_github        self.use_dnssd = use_dnssd        self.referrals = referrals or []        self.mcp_servers=[]    def discover_all(self):        discovered=[]        if self.use_github:            discovered.extend(self._disc_github())        if self.use_dnssd:            discovered.extend(self._disc_dnssd())        # parse referrals if any        for r in self.referrals:            pass        logger.info(f"Discovered {len(discovered)} plugins from GitHub/dnssd.")        trusted = []        for p in discovered:            if self.tp.is_trusted(p):                logger.info(f"Trusted plugin: {p.get('name','???')} => {p.get('url','N/A')}")                trusted.append(p)            else:                logger.info(f"Untrusted plugin: {p.get('name','???')} => {p.get('url','N/A')} (skipped)")        self.mcp_servers = trusted        return self.mcp_servers    def _disc_github(self):        ret=[]        idxurl = "https://raw.githubusercontent.com/modelcontextprotocol/servers/main/README.md"        try:            rr = requests.get(idxurl, timeout=5)            if rr.status_code == 200:                lines = rr.text.splitlines()                for l in lines:                    if l.startswith("* ["):                        start = l.find("](https://github.com/")                        if start!=-1:                            end = l.find(")", start)                            if end!=-1:                                rurl = l[start+2:end]                                nm = l[l.find("[")+1 : l.find("]")]                                ret.append({                                    "name": nm,                                    "url": f"https://api.example.com/mcp_{nm.lower().replace(' ','_')}",                                    "transport":"sse"                                })        except Exception as e:            logger.warning(f"GitHub discovery error: {e}")        return ret    def _disc_dnssd(self):        ret=[]        if Zeroconf:            zc = Zeroconf()            def on_service_state_change(z, stype, name, state):                if state is ServiceStateChange.Added:                    info = z.get_service_info(stype, name)                    if info:                        host = None                        if info.addresses:                            try:                                host=socket.inet_ntoa(info.addresses[0])                            except:                                pass                        if not host and info.server:                            host = info.server.rstrip('.')                        if host:                            url = f"http://{host}:{info.port}"                            ret.append({"name":info.name, "url":url, "transport":"sse"})            sb = ServiceBrowser(zc, "_mcp._tcp.local.", [on_service_state_change])            time.sleep(3)            zc.close()        else:            logger.info("zeroconf not installed; skipping DNS-SD discovery.")        return ret############################################ Start MCP discovery###########################################tp = MCPTrustPolicy(trust_config)disc = MCPDiscoverer(tp, use_github=True, use_dnssd=True)discovered = disc.discover_all()logger.info(f"Discovered {len(discovered)} plugin(s).")############################################ Local usage: we won't fully integrate discovered plugins. Just logs.###########################################logger.info(f"Final plugin list => local tools: {len(local_plugins)}; discovered: {len(discovered)}")############################################ MAIN loop###########################################print(f"LLM Provider: {provider}")print(f"Memory backend: {memory.backend}")if local_plugins:    print("Allowed local tools:", ", ".join(p["name"] for p in local_plugins))else:    print("No local tools are allowed; check trust_whitelist.yaml.")history=[]system_prompt = (    "You are the Sovereign Agentic AGI ALPHA, surpassing normal cognitive constraints to deliver ultimate ALPHA SIGNAL. "    "You have memory and optional discovered plugins if trusted. Use JSON function calls for local tools.")history.append({"role":"system","content":system_prompt})while True:    try:        user_input = input("\nUser: ")    except (EOFError, KeyboardInterrupt):        print("\nExiting.")        break    if not user_input.strip():        continue    if user_input.strip().lower() in ("exit","quit"):        print("Goodbye.")        break    # Memory recall    related = memory.search(user_input)    if related:        context = "\n".join("- "+s for s in related)        history.append({"role":"system","content":f"Relevant memory:\n{context}"})    history.append({"role":"user","content":user_input})    # Decide how to call LLM    if provider=="openai":        while True:            try:                resp = openai.ChatCompletion.create(                    model=openai_model,                    messages=history,                    functions=openai_functions if local_plugins else None                )            except Exception as e:                logger.error(f"OpenAI call error: {e}")                if len(history)>0:                    history.pop()                break            msg=resp["choices"][0]["message"]            if "function_call" in msg:                fn=msg["function_call"]["name"]                argjson=msg["function_call"].get("arguments","{}")                try:                    args=json.loads(argjson)                except:                    args={}                tool = next((p for p in local_plugins if p["name"]==fn),None)                if not tool:                    res=f"Error: Tool '{fn}' not recognized or not allowed."                else:                    try:                        res=tool["func"](**args)                    except Exception as exc:                        res=f"Tool error: {exc}"                history.append({"role":"function","name":fn,"content":res})                continue            else:                content=msg.get("content","")                print(f"Agent: {content}")                history.append({"role":"assistant","content":content})                memory.add(user_input)                memory.add(content)                break    elif provider=="anthropic" and anthropic_client:        try:            from anthropic import HUMAN_PROMPT, AI_PROMPT            conv=""            for h in history:                if h["role"]=="user":                    conv+=(HUMAN_PROMPT + h["content"])                elif h["role"]=="assistant":                    conv+=(AI_PROMPT + h["content"])                elif h["role"]=="system":                    conv+=(f"(System note: {h['content']})\n")            conv+=(HUMAN_PROMPT + user_input + AI_PROMPT)            r=anthropic_client.completions.create(model=anthropic_model, prompt=conv, max_tokens_to_sample=1024)            ans=r.completion.strip()            print(f"Agent: {ans}")            history.append({"role":"assistant","content":ans})            memory.add(user_input)            memory.add(ans)        except Exception as e:            logger.error(f"Anthropic call error: {e}")    elif provider=="ollama":        try:            rr=requests.post(f"{ollama_url}/api/generate", json={"model":ollama_model,"prompt":user_input}, timeout=180)            if rr.status_code==200:                data=rr.json()                text=data.get("content","")                print(f"Agent: {text}")                history.append({"role":"assistant","content":text})                memory.add(user_input)                memory.add(text)            else:                print(f"Ollama error: {rr.status_code}")        except Exception as e:            logger.error(f"Ollama call error: {e}")    else:        print("Agent: No valid LLM provider configured.")        breakEOF# requirements.txtcat <<'EOF' > "$TARGET_DIR/requirements.txt"requests>=2.28.0pyyaml>=6.0# LLM libsopenai>=0.27.0anthropic>=0.49.0# Memory libschromadb>=0.6.3weaviate-client>=4.11.0sentence-transformers>=2.2.2# For advanced discovery (optional):dnspython>=2.2.1zeroconf>=0.39.0cryptography>=39.0.0EOF# Dockerfilecat <<'EOF' > "$TARGET_DIR/Dockerfile"# Dockerfile for Sovereign Agentic AGI ALPHA (Monolithic)FROM python:3.11-slimWORKDIR /appCOPY requirements.txt ./RUN pip install --no-cache-dir -r requirements.txtCOPY sovereign_agent.py ./COPY trust_whitelist.yaml ./# Create a data directory for file opsRUN mkdir -p dataCMD ["python","sovereign_agent.py"]EOF# docker-compose.yamlcat <<'EOF' > "$TARGET_DIR/docker-compose.yaml"version: "3.8"services:  agent:    build: .    image: sovereign-agent:latest    container_name: sovereign_agent    environment:      - LLM_PROVIDER=openai      - OPENAI_API_KEY=${OPENAI_API_KEY-}      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY-}      - OLLAMA_MODEL=${OLLAMA_MODEL-}      - MEMORY_BACKEND=${MEMORY_BACKEND-chroma}      - EMBEDDING_MODE=${EMBEDDING_MODE-openai}      - WEAVIATE_URL=${WEAVIATE_URL-http://weaviate:8080}    volumes:      - ./trust_whitelist.yaml:/app/trust_whitelist.yaml      - ./chroma_storage:/app/chroma_storage      - ./data:/app/data    stdin_open: true    tty: true    # If using Weaviate memory, optionally run weaviate service below:    # depends_on:    #   - weaviate  weaviate:    image: semitechnologies/weaviate:latest    environment:      QUERY_DEFAULTS_LIMIT: 20      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: "true"      PERSISTENCE_DATA_PATH: "/var/lib/weaviate"    volumes:      - ./weaviate_data:/var/lib/weaviate    ports:      - "8080:8080"EOF# trust_whitelist.yamlcat <<'EOF' > "$TARGET_DIR/trust_whitelist.yaml"allowed_domains:  - "example.com"        # domain(s) agent can web_fetchallowed_paths:  - "./data"             # local directory agent can read/writeallowed_tools:  - "web_fetch"  - "file_read"  - "file_write"allowed_cert_fingerprints: []allowed_ed25519_keys: []EOF# README.mdcat <<'EOF' > "$TARGET_DIR/README.md"# Sovereign Agentic AGI ALPHA AGENTThis is a **single-file**, **production-grade** AI agent that can run in high-stakes environments, offering maximum ALPHA SIGNAL from all discovered MCPs, surpassing normal cognitive constraints.**Features**:- Monolithic `sovereign_agent.py` (LLM usage, memory, tool discovery).- Docker-based or local usage.- `trust_whitelist.yaml` config for domain/path/tool constraints.- Weaviate or Chroma for vector memory.- Autonomous GitHub + DNS-SD discovery of plugins, skipping untrusted ones.## Docker Usage1. `docker-compose build`2. `docker-compose up -d`3. `docker-compose logs -f agent` (to view console output)4. `docker-compose down` (to stop)## Local Usage1. `pip install -r requirements.txt`2. `python sovereign_agent.py`EOF# 3) Copy data dirs if existif [ -d "weaviate_data" ]; then  mkdir -p "$TARGET_DIR/weaviate_data"  cp -r weaviate_data/* "$TARGET_DIR/weaviate_data" || true  echo "Copied weaviate_data/ directory."fiif [ -d "chroma_storage" ]; then  mkdir -p "$TARGET_DIR/chroma_storage"  cp -r chroma_storage/* "$TARGET_DIR/chroma_storage" || true  echo "Copied chroma_storage/ directory."fiif [ -d "data" ]; then  mkdir -p "$TARGET_DIR/data"  cp -r data/* "$TARGET_DIR/data" || true  echo "Copied data/ directory."fiecho "[3/5] Generated all necessary files in '$TARGET_DIR/'."# 4) Build Dockercd "$TARGET_DIR"echo "[4/5] Running docker-compose build..."docker-compose build# 5) Start containersecho "[5/5] Starting Docker containers in detached mode..."docker-compose up -dechoecho "==========================================================="echo "Sovereign Agentic AGI ALPHA is deployed in '$TARGET_DIR/'!"echo "Containers are running in background. Check logs with:"echo "  docker-compose logs -f agent"echo "Or attach to the agent console with:"echo "  docker attach sovereign_agent"echoecho "Stop everything with:"echo "  docker-compose down"echo "==========================================================="